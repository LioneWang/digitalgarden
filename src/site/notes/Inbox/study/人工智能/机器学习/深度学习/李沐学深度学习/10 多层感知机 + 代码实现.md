---
{"dg-publish":true,"tags":["deep_learning","course"],"permalink":"/Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/10 多层感知机 + 代码实现/","dgPassFrontmatter":true}
---



# 10 多层感知机 + 代码实现
- 单层感知机
    ![tmp-79.png](/img/user/Assets/attachments/tmp/tmp-79.png)
    - 和线性回归相比：它的输出是不离散的
	- 和softmax回归相比：它要解决二分类问题
	- 算法
	![tmp-80.png](/img/user/Assets/attachments/tmp/tmp-80.png)
	- 超参数：隐藏层输出大小
	- 局限：不能满足XOR问题
- 多层感知机（隐藏层的激活函数一定要是非线性的，否则就等价于单层感知机）
	- 单隐藏层，单分类（输出就一个神经元）
		- 激活函数：
			- [[Inbox/tools/sigmoid\|sigmoid]]——区别就在于sigmoid函数比较平滑，因此称作软的
			![tmp-81.png](/img/user/Assets/attachments/tmp/tmp-81.png)
			- tanh函数
			![tmp-82.png](/img/user/Assets/attachments/tmp/tmp-82.png)
			- [[Inbox/tools/relu\|relu]]函数
			![tmp-83.png](/img/user/Assets/attachments/tmp/tmp-83.png)
			- 三个激活函数的对比：relu运算最简单，而sigmoid和tanh函数都有指数运算。在CPU里做一次指数运算相当于算100次乘法运算；而GPU里面有专门的指数计算单元，所以好一点
	- 多分类
    	![tmp-84.png](/img/user/Assets/attachments/tmp/tmp-84.png)
		- 和softmax回归的对比：mlp加了隐藏层和非线性激活函数
		- 超参数：隐藏层层数和每层隐藏层的输出大小

---
# References
🔗[10 多层感知机 + 代码实现 - 动手学深度学习v2_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1hh411U7gn/?spm_id_from=333.1387.collection.video_card.click&vd_source=73a67190a2e14f51c71c0fa447f094aa)