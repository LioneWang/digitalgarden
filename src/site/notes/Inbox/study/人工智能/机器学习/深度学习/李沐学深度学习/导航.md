---
{"tags":["deep_learning","course"],"dg-publish":true,"permalink":"/Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/导航/","dgPassFrontmatter":true}
---

# 导航

![tmp-250.png](/img/user/Assets/attachments/tmp/tmp-250.png)

## 🔗深度学习基础
### 🍉线性模型
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/05 线性代数\|线性回归]]：线性模型的回归任务
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/09 Softmax 回归 + 损失函数 + 图片分类数据集\|Softmax回归]]：线性模型的分类任务
### 🌽非线性模型
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/10 多层感知机 + 代码实现\|多层感知机]]：非线性模型，非线性由激活函数表现
### 🥕基本知识
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/11 模型选择 + 过拟合和欠拟合\|过拟合和欠拟合]]：衡量模型的指标
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/12 权重衰退\|权重衰退]]：L1和L2正则化，针对权重的正则化
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/13 丢弃法\|13 丢弃法]]：留出法，针对权重的正则化
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/14 数值稳定性 + 模型初始化和激活函数\|14 数值稳定性 + 模型初始化和激活函数]]：针对梯度消失和梯度爆炸的问题，采用针对数值的ln和bn归一化和针对权重的Xavier归一化
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/14 数值稳定性 + 模型初始化和激活函数\|14 数值稳定性 + 模型初始化和激活函数]]：三种激活函数——Relu，Tanh和Sigmoid
## 🌐卷积神经网络
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/23 经典卷积神经网络 LeNet\|LeNet]]：卷积+池化+全连接层
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/24 深度卷积神经网络 AlexNet\|AlexNet]]：相比LeNet，加入留出法；激活函数变成relu函数；平均池化变成max池化
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/25 使用块的网络 VGG\|VGG]]：相比AlexNet，采用更深更窄的网络，vgg块通常由3×3卷积，激活层，池化层组成
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/26 网络中的网络 NiN\|NiN]]：相比前三个CNN网络，采用1×1卷积层替代全连接层，显著降低了参数量；采用全局平均池化代替最后的输出层
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/27 含并行连结的网络 GoogLeNet -Inception V3\|GoogleNet]]：采用NiN块，每个Inception块用四个分支完成
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/29 残差网络 ResNet\|ResNet]]：每个ResNet块采用跳跃连接的方式，第一个残差块将高宽减半通道加倍；后面的残差块维持高度不变
## 👓计算机视觉
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/36 数据增广\|图片增广]]：图片的预处理操作，包括裁剪，翻转和颜色变化
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/37 微调\|微调]]：用Imagenet数据集训练过的Resnet作为预训练网络，完成后续任务，改变输出层权重
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/44 物体检测算法：R-CNN，SSD，YOLO\|R-CNN，SSD和YOLO]]：目标检测锚框的三种算法，R-CNN用到Roi兴趣池化层
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/48 全连接卷积神经网络 FCN\|FCN]]：语义分割的方法，用于像素级的分类问题，先用预训练模型得到特征，之后用1×1卷积降维，最后用转置卷积还原图像形状
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/49 样式迁移\|样式迁移]]
## ♻️循环神经网络
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/54 循环神经网络 RNN\|RNN]]：解决文本分类问题的时序性信息的一种算法，通常采用潜变量模型更新状态
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/57 长短期记忆网络（LSTM）\|LSTM]]：解决了RNN对长距离较敏感的问题，引入输入门，遗忘门，输出门和记忆单元
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/56 门控循环单元（GRU）\|GRU]]：LSTM的简化版，只用更新门和重置门
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/58 深层循环神经网络\|深层RNN]]：在RNN基础上增加隐藏层的层数
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/59 双向循环神经网络\|双向RNN]]：在RNN单向的基础上加一个反向RNN，用双向RNN更新当前位置的潜变量
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/62 序列到序列学习（seq2seq）\|Seq2Seq]]：RNN基础上的Encoder-Decoder结构，Encoder负责输入source，Decoder负责预测输出
## 👽注意力机制
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/66 使用注意力机制的seq2seq\|Seq2seq+attention]]：注意力机制解决了RNN的关于长距离依赖的问题，用query，key计算相似度得到每个时间步的不同权重再和value做乘法
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/68 Transformer\|Transformer]]：在自注意力机制基础上，引入多头注意力机制，FFN前馈神经网络，残差链接+LN，提高并行度
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/69 BERT预训练\|BERT]]：在Transformer基础上，只保留Encoder，因为它是一个预训练模型，无法做预测
## 🧩性能
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/31 深度学习硬件：CPU 和 GPU\|CPU和GPU]]：模型的参数存储在内存中，模型的参数做反向传播和运算在CPU或GPU的寄存器中
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/34 多GPU训练实现\|34 多GPU训练实现]]：数据并行的方式，将数据存放在不同GPU中
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/35 分布式训练\|分布式]]：用分布式文件系统存储样本，多个worker工作站存储gpu，参数服务器存储参数
## 📱应用
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/15  实战：Kaggle房价预测 + 课程竞赛：加州2020年房价预测【动手学深度学习v2】\|房价预测]]
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/09 Softmax 回归 + 损失函数 + 图片分类数据集\|图片分类]]
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/41 物体检测和数据集\|物体检测]]
- [[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/70 BERT微调\|BERT]]

# References
Inbox/Resources/李沐动手学深度学习
cd E:\obsidian\Rainbow-Components\Inbox\resources\李沐动手学深度学习\pytorch

[跟李沐学AI的个人空间-跟李沐学AI个人主页-哔哩哔哩视频](https://space.bilibili.com/1567748478/lists/358497?type=series)