---
{"dg-publish":true,"tags":["machine_learning","deep_learning","course"],"permalink":"/Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/14 数值稳定性 + 模型初始化和激活函数/","dgPassFrontmatter":true}
---


# 14 数值稳定性 + 模型初始化和激活函数
- 梯度爆炸
	- mlp的例子，每个层的导数做链式乘积，如果每个值大于1，那么层数很大的话就很大
	- 问题
		- 如果导数很大的话，而gpu通常用16位浮点数，会超过这个值，导致出现问题
		- 对学习率过于敏感
- 梯度消失
	- 用sigmoid作为激活函数[[Inbox/study/人工智能/机器学习/深度学习/李沐学深度学习/10 多层感知机 + 代码实现\|10 多层感知机 + 代码实现]]
	- 问题
		- 16位浮点数会当成0
		- 训练没有进展，不管如何选择学习率
- 解决办法
	- 小的网络：权重归一化，均值为0方差为1[[Inbox/wiki/Resnet\|Resnet]]
	- 大的网络：Xavier初始化，使得输入和输出的方差相同

---
# References
[14 数值稳定性 + 模型初始化和激活函数【动手学深度学习v2】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1u64y1i75a/?spm_id_from=333.1387.collection.video_card.click&vd_source=73a67190a2e14f51c71c0fa447f094aa)