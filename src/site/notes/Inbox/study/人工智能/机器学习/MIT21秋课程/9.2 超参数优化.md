---
{"dg-publish":true,"tags":["MIT","course","machine_learning","deep_learning"],"permalink":"/Inbox/study/人工智能/机器学习/MIT21秋课程/9.2 超参数优化/","dgPassFrontmatter":true}
---


# 9.2 超参数优化
- 搜索空间
![tmp-51.png](/img/user/Assets/attachments/tmp/tmp-51.png)
	- 模型骨架：从小的网络到大的网络
	- 学习率：
	- 批量大小：
	- momentum（SGD参数）
	- weight decay（正则化技术）
- HPO算法
	- 黑盒：遍历每个超参数组合看准确率
	- multi-fidelity（多置信）
		- 减少数据集数量
		- 减少模型大小（用更少的层数和通道数）
		- 早停止（当观察到曲线不好的时候就停止训练）
- 算法详细介绍：
![tmp-52.png](/img/user/Assets/attachments/tmp/tmp-52.png)
	- 黑盒优化
		- 梯度搜索
		![tmp-53.png](/img/user/Assets/attachments/tmp/tmp-53.png)
		- 随机搜索
		![tmp-54.png](/img/user/Assets/attachments/tmp/tmp-54.png)
		- 贝叶斯优化
	- 多置信优化
		- successive halving：随机选n个超参数组来训练m个epoch
		![tmp-55.png](/img/user/Assets/attachments/tmp/tmp-55.png)
		- hyperband：用多个SH算法重复
		![tmp-56.png](/img/user/Assets/attachments/tmp/tmp-56.png)
- 总结
	- 你可以用论文中的已知的超参数来做，意识到存在top performers
---
# References
[9.2 超参数优化【斯坦福21秋季：实用机器学习中文版】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1FM4y1c7yG?spm_id_from=333.788.player.switch&vd_source=73a67190a2e14f51c71c0fa447f094aa)