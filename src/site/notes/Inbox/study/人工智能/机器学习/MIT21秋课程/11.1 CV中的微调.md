---
{"dg-publish":true,"tags":["machine_learning","deep_learning","MIT","course"],"permalink":"/Inbox/study/人工智能/机器学习/MIT21秋课程/11.1 CV中的微调/","dgPassFrontmatter":true}
---



# 11.1 CV中的微调
- 动机
	- 利用一个在单一任务上训练的模型来完成另一件相关的任务
	- 在深度学习中很火爆因为DNN是非常数据饥渴的并且训练成本非常高
- 方法
	- 特征抽取（word2vec，ResNet-50，I3D）
	- 在一个模型中训练然后重用
	- 从一个预训练模型中微调[11.1 CV中的微调](11.1%20CV中的微调.md)
		- 神经网络的分工
			- 编码器：把输入的像素转化成语义可分的特征
			- 解码器：映射成标号做决策
		- 微调
			- 找到一个预训练模型，初始化编码器中的所有权重（权重复制预训练过的权重）
				- 哪里找预训练模型：
					- [Find Pre-trained Models | Kaggle](https://www.kaggle.com/models?tfhub-redirect=true)
					- [huggingface/pytorch-image-models: The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more](https://github.com/huggingface/pytorch-image-models)
				- 怎么用微调的模型训练：
					- 确定网络架构（如果要低延迟在手机上部署用mobilenet；）
					- 确定预训练模型在哪个数据集上训练
					- 将图片4D特征变成2D[10.1 深度神经网络架构](10.1%20深度神经网络架构.md)
			- 随机初始化输出层（输出层，目的是为了在新任务中做的更好）
			- 开始权重优化，但是由于目前权重已经比较接近最优解了，所以可以限制一下学习率同时限制轮数（防止离最优权重太远）
				- 将搜索空间变小（假设预训练是一个比较大的数据集，当前任务需要在一个小数据集上表现很好，如果不限制搜索空间的话可能会过拟合，因此可以限制搜索空间在一个特定的范围防止过拟合）
				- 冻结底部的层：因为越底下的层学习的特征越局部，越往上学习的特征越全局。可以固定前几层的学习率是0，逐渐增加每层的学习率。

---
# References
[11.1 迁移学习【斯坦福21秋季：实用机器学习中文版】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1SL4y1t7ZL/?spm_id_from=333.1387.collection.video_card.click&vd_source=73a67190a2e14f51c71c0fa447f094aa)