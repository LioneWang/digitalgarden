---
{"dg-publish":true,"tags":["deep_learning","machine_learning","MIT","course"],"permalink":"/Inbox/study/人工智能/机器学习/MIT21秋课程/3.7 RNN/","dgPassFrontmatter":true}
---



![tmp-36.png](/img/user/Assets/attachments/tmp/tmp-36.png)
- [[Inbox/wiki/RNN\|RNN]]和Mlp的唯一区别就是多了一个记忆的参数，这个记忆的参数指的是上一次经过全连接层的倒数第二层的信息
![tmp-37.png](/img/user/Assets/attachments/tmp/tmp-37.png)
- 注意w_xh和w_hh其实是同一个隐藏层，只不过输入不一样，但是输出是一样的；tanh是激活函数
---
# References
[3.7 循环神经网络【斯坦福21秋季：实用机器学习中文版】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pq4y1r7zo?spm_id_from=333.788.videopod.sections&vd_source=73a67190a2e14f51c71c0fa447f094aa)