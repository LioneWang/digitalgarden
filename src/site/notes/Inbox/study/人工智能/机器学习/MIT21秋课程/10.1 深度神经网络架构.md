---
{"dg-publish":true,"tags":["deep_learning","machine_learning","MIT","course"],"permalink":"/Inbox/study/人工智能/机器学习/MIT21秋课程/10.1 深度神经网络架构/","dgPassFrontmatter":true}
---


# 10.1 深度神经网络架构
- 深度神经网络其实是一门语言
- DL设计模式：
	- 批量和层的归一化
		- 批量归一化
			- 数据归一化(线性模型)：作用是可以使得loss函数更加平滑，允许很大的学习率。因为当输入数据相差很大的时候，如果求当前位置的导数，当学习率很大的时候，可能下一步距离这个函数会非常的远
			- 批量归一化（深度神经网络）
				- 步骤
					- reshape：将输入（图片是4D数据，文本是2D数据）变成2D的
					- normalize：标准化输入
					- recovery：恢复标准化之前的输入（其实不是为了恢复，而是说归一化后的数据可能对于特定任务的表现能力不好，因此可训练参数的作用是为了让这个参数更加贴近于任务）（用于minibatch中）
					- 还原维度：将2D数据还原为原来的维度（训练后）
					![tmp-58.png](/img/user/Assets/attachments/tmp/tmp-58.png)
		- 层归一化：将每个样本中的特征做归一化，其他和BN一样（eg：transformer使用ln而不是bn）
		- 其他归一化方法
			- 修改“reshape”：
			- 修改“normalize”：whitening，在归一化的基础上做一个[[Inbox/wiki/PCA\|PCA]]
			- 修改“recovery”：把线性模型变成全连接层和神经网络
			- 权重和梯度的标准化
	- 残差连接[【精度】Resnet（2015）](【精度】Resnet（2015）.md)
	- 注意力机制[【精度】Transformer（2017）](【精度】Transformer（2017）.md)

---
# References
[10.1 深度神经网络架构【斯坦福21秋季：实用机器学习中文版】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1WY411p7Zp?spm_id_from=333.788.videopod.sections&vd_source=73a67190a2e14f51c71c0fa447f094aa)