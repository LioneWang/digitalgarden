---
{"dg-publish":true,"tags":["deep_learning","machine_learning","MIT","course"],"permalink":"/Inbox/study/人工智能/机器学习/MIT21秋课程/5.2 Bagging/","dgPassFrontmatter":true}
---


- n个模型并行训练
- 如果是回归问题，平均所有的output；如果是分类问题，选出大部分的投票结果
- 每个学习器（决策树，mlp，神经网络）通过bootstrap sampling进行采样
	- 假设有m个训练样本，之后随机取m个样本（可以重复）
	- 大概每个模型会取得63%个样本；其余37%个样本是重复情况
![tmp-47.png](/img/user/Assets/attachments/tmp/tmp-47.png)
- 通过bagging降低了方差，因此泛化误差会变小
- 决策树本来就不太稳定，因此做bagging效果更好；线性模型本来就稳定，因此做bagging效果变化不明显
---
# References
[5.2 Bagging【斯坦福21秋季：实用机器学习中文版】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV13g411N7xy/?spm_id_from=333.1387.collection.video_card.click&vd_source=73a67190a2e14f51c71c0fa447f094aa)